{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682c4c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe84af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e461e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-20b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f93f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Not much! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 13, 'total_tokens': 39, 'completion_time': 0.024077797, 'prompt_time': 0.003282625, 'queue_time': 0.049535185, 'total_time': 0.027360422}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a3d9ee71-fceb-463a-bdc9-21bb23fb69a6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 26, 'total_tokens': 39})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model call\n",
    "\n",
    "model.invoke(\"Whats up, Kush here?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42416cbd",
   "metadata": {},
   "source": [
    "[Exercise] Play along. Give bigger and bigger prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2738af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm LLaMA, a large language model trained by a team of researcher at Meta AI. My primary function is to understand and generate human-like text based on the input I receive. I can answer questions, engage in conversations, provide information, and even create creative content like stories or poems.\\n\\nI'm a type of AI designed to process and understand natural language, which means I can understand and respond to questions and prompts in a way that's similar to how a human would. I've been trained on a massive dataset of text from the internet and can draw upon that knowledge to provide information on a wide range of topics.\\n\\nI'm constantly learning and improving, so the more conversations I have, the better I become at understanding and responding to questions. I'm here to help answer any questions you may have, provide information on a topic you're interested in, or simply chat with you about your day. What would you like to talk about?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 14, 'total_tokens': 205, 'completion_time': 0.177864567, 'prompt_time': 0.00303111, 'queue_time': 0.04958096, 'total_time': 0.180895677}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4d0e638e-cbd5-4cb7-a397-6d415f56b640-0', usage_metadata={'input_tokens': 14, 'output_tokens': 191, 'total_tokens': 205})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Who are you and where are you located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f4578d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a Python code that multiplies two matrices of arbitrary order and throws an exception if the matrix dimensions are not compatible for multiplication:\\n```\\nimport numpy as np\\n\\ndef multiply_matrices(mat1, mat2):\\n    # Check if matrices are empty\\n    if not mat1.shape[0] or not mat2.shape[1]:\\n        raise ValueError(\"Matrices cannot be empty\")\\n\\n    # Check if matrices are compatible for multiplication\\n    if mat1.shape[1] != mat2.shape[0]:\\n        raise ValueError(\"Matrices are not compatible for multiplication\")\\n\\n    # Multiply matrices\\n    result = np.dot(mat1, mat2)\\n\\n    return result\\n\\n# Example usage:\\nmat1 = np.array([[1, 2], [3, 4]])\\nmat2 = np.array([[5, 6], [7, 8]])\\nresult = multiply_matrices(mat1, mat2)\\nprint(result)\\n```\\nHere\\'s an explanation of the code:\\n\\n1. We check if the matrices are empty by checking if the number of rows in the first matrix and the number of columns in the second matrix are both 0. If either of them is 0, we raise a `ValueError`.\\n2. We check if the matrices are compatible for multiplication by checking if the number of columns in the first matrix is equal to the number of rows in the second matrix. If they are not equal, we raise a `ValueError`.\\n3. If the matrices are compatible, we use the `np.dot` function to multiply them. This function performs the matrix multiplication and returns the result.\\n4. We return the result of the matrix multiplication.\\n\\nYou can test this code with different matrices to see that it works correctly and throws an exception when the matrices are not compatible for multiplication.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 362, 'prompt_tokens': 43, 'total_tokens': 405, 'completion_time': 0.335661644, 'prompt_time': 0.010243939, 'queue_time': 0.058644891, 'total_time': 0.345905583}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--71f72909-d18c-496f-b365-4254c5bd0286-0', usage_metadata={'input_tokens': 43, 'output_tokens': 362, 'total_tokens': 405})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"\"\" Write a python code which can multiply two matrix of arbitrary but compatible order. The code should throw exception if the matrix dimentsions are not compatible for multiplication             \n",
    "             \"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c8c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python code that multiplies two matrices of arbitrary order and throws an exception if the matrix dimensions are not compatible for multiplication:\n",
      "```\n",
      "import numpy as np\n",
      "\n",
      "def multiply_matrices(mat1, mat2):\n",
      "    # Check if matrices are empty\n",
      "    if not mat1.shape[0] or not mat2.shape[1]:\n",
      "        raise ValueError(\"Matrices cannot be empty\")\n",
      "\n",
      "    # Check if matrices are compatible for multiplication\n",
      "    if mat1.shape[1] != mat2.shape[0]:\n",
      "        raise ValueError(\"Matrices are not compatible for multiplication\")\n",
      "\n",
      "    # Multiply matrices\n",
      "    result = np.dot(mat1, mat2)\n",
      "\n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "mat1 = np.array([[1, 2], [3, 4]])\n",
      "mat2 = np.array([[5, 6], [7, 8]])\n",
      "result = multiply_matrices(mat1, mat2)\n",
      "print(result)\n",
      "```\n",
      "Here's an explanation of the code:\n",
      "\n",
      "1. We check if the matrices are empty by checking if the number of rows in the first matrix and the number of columns in the second matrix are both 0. If either of them is 0, we raise a `ValueError`.\n",
      "2. We check if the matrices are compatible for multiplication by checking if the number of columns in the first matrix is equal to the number of rows in the second matrix. If they are not equal, we raise a `ValueError`.\n",
      "3. If the matrices are compatible, we use the `np.dot` function to multiply them. This function performs the matrix multiplication and returns the result.\n",
      "4. We return the result of the matrix multiplication.\n",
      "\n",
      "You can test this code with different matrices to see that it works correctly and throws an exception when the matrices are not compatible for multiplication.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b215da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='নমস্কার! (Namaskar!)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41, 'completion_time': 0.01494872, 'prompt_time': 0.003708529, 'queue_time': 0.049285161, 'total_time': 0.018657249}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ca3e7989-12e6-4fb0-af3c-6a1b31ebdfc0-0', usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking to build a converstation style call\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Greek\"), # try punjabi, or any other Indian language\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9efe5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Generate python code for given tasks\"),\n",
    "    HumanMessage(\"Find max of given n numbers\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d70ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code to find the maximum of n numbers:\n",
      "\n",
      "```Python\n",
      "def find_max(n, numbers):\n",
      "    max_num = numbers[0]\n",
      "    for num in numbers:\n",
      "        if num > max_num:\n",
      "            max_num = num\n",
      "    return max_num\n",
      "\n",
      "# test the function\n",
      "n = int(input(\"Enter number of elements: \"))\n",
      "numbers = [int(input(\"Enter element {}: \".format(i+1))) for i in range(n)]\n",
      "print(\"Maximum number is: \", find_max(n, numbers))\n",
      "```\n",
      "\n",
      "In this code, we first take the input of the number of elements and the elements themselves. Then we use a function `find_max` to find the maximum number. The function initializes the maximum number as the first number in the list, and then iterates over the rest of the numbers. If it finds a number greater than the current maximum, it updates the maximum. Finally, it returns the maximum number.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d36098aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code to find the maximum of n numbers:\n",
      "\n",
      "```Python\n",
      "def find_max(n, numbers):\n",
      "    max_num = numbers[0]\n",
      "    for num in numbers:\n",
      "        if num > max_num:\n",
      "            max_num = num\n",
      "    return max_num\n",
      "\n",
      "# test the function\n",
      "n = int(input(\"Enter number of elements: \"))\n",
      "numbers = [int(input(\"Enter element {}: \".format(i+1))) for i in range(n)]\n",
      "print(\"Maximum number is: \", find_max(n, numbers))\n",
      "```\n",
      "\n",
      "In this code, we first take the input of the number of elements and the elements themselves. Then we use a function `find_max` to find the maximum number. The function initializes the maximum number as the first number in the list, and then iterates over the rest of the numbers. If it finds a number greater than the current maximum, it updates the maximum. Finally, it returns the maximum number.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39caa649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content='Hello' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' It' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=\"'s\" additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' nice' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' meet' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' Is' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' there' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' something' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' help' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' with' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' or' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' would' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' like' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content=' chat' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand'} id='run--7f6724c1-fac1-407d-ae06-04eb0ff0048a' usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37}\n"
     ]
    }
   ],
   "source": [
    "# streaming example\n",
    "import time\n",
    "\n",
    "for token in model.stream(\"hello\"):\n",
    "    time.sleep(0.1)\n",
    "    #print(token.content, end=\"|\")\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f083693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x000002A624696C20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stream(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6837d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class discussion point: What is an LLM as a program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca534bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
