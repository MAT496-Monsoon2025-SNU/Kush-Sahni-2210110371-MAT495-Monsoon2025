{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ec513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45e179",
   "metadata": {},
   "source": [
    "## What is RAG (retrieval augmented generation)?\n",
    "Basically, shoving lot of extra information in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52bc9487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Ajit is Preeti's brother. He is also the brother of Sweta. There is no other information about any other brothers of Preeti.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 119, 'total_tokens': 152, 'completion_time': 0.130439884, 'prompt_time': 0.006059703, 'queue_time': 0.070203587, 'total_time': 0.136499587}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--f45e7158-a133-4d6c-9411-1fb346c71e75-0' usage_metadata={'input_tokens': 119, 'output_tokens': 33, 'total_tokens': 152}\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(\n",
    "    prompt_template.format(\n",
    "        context=\"Ajit has two sisters, Preeti and Sweta. Ajit is male.\",\n",
    "        question=\"Who is Preeti's bother?\"         \n",
    "    ))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475aef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajit is Preeti's brother. He is also the brother of Sweta. There is no other information about any other brothers of Preeti.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33703cf0",
   "metadata": {},
   "source": [
    ">RAG is all about cleverly pushing is as much information in the context with minimum possible tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b38ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little non-trival RAG example\n",
    "\n",
    "# grab a novel\n",
    "file = open(\"elonmusk.txt\", encoding=\"utf-8\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07503397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='इस संग्रह में केवल एक कहानी है, जो एलोन मस्क के बारे में है। यहाँ कहानी का शीर्षक और सारांश है:\\n\\n1. एलोन मस्क\\nएलोन मस्क एक प्रमुख उद्यमी और व्यवसायी हैं, जिन्होंने जनवरी 2025 से मई 2025 तक वरिष्ठ सलाहकार के रूप में कार्य किया। उनका जन्म एलोन रीवे मस्क के रूप में हुआ था। यह कहानी उनके जीवन और कार्यों के बारे में जानकारी प्रदान करती है।' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 234, 'total_tokens': 396, 'completion_time': 0.382006576, 'prompt_time': 0.020906186, 'queue_time': 0.113745484, 'total_time': 0.402912762}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5ea1c369-6e4e-47f9-9a2d-9d2d32ce6c68-0' usage_metadata={'input_tokens': 234, 'output_tokens': 162, 'total_tokens': 396}\n"
     ]
    }
   ],
   "source": [
    "# lets try to talk with this book\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "# Prepare your prompt\n",
    "prompt_template = \"\"\"You are a novel reader. You are given collection of stories:\n",
    "{collection_of_stories}\n",
    "You are tasked to make a list of story titles in this collection. Write a short summary for each story in Hindi Language. Skip the story from the list, if the story is not provided in the text. \n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(collection_of_stories = text[:len(text)//20]))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87249ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "इस संग्रह में केवल एक कहानी है, जो एलोन मस्क के बारे में है। यहाँ कहानी का शीर्षक और सारांश है:\n",
       "\n",
       "1. एलोन मस्क\n",
       "एलोन मस्क एक प्रमुख उद्यमी और व्यवसायी हैं, जिन्होंने जनवरी 2025 से मई 2025 तक वरिष्ठ सलाहकार के रूप में कार्य किया। उनका जन्म एलोन रीवे मस्क के रूप में हुआ था। यह कहानी उनके जीवन और कार्यों के बारे में जानकारी प्रदान करती है।"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better printing\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7239b",
   "metadata": {},
   "source": [
    "### Problem: Too much raw information in the context makes the prompt too long.\n",
    "- Costly\n",
    "- adds noise\n",
    "### Solution: Use RAG\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "### To understand RAG, we need to understand Semantic Search\n",
    "https://python.langchain.com/docs/tutorials/retrievers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ca6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text using RELEVANT loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"elonmusk.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50eab39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split given book into 14 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# Split document into small chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split given book into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dd2b5",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758a909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_google_genai in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (2.1.12)\n",
      "Requirement already satisfied: langchain-core>=0.3.75 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain_google_genai) (0.3.76)\n",
      "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain_google_genai) (2.11.7)\n",
      "Requirement already satisfied: filetype<2,>=1.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (6.32.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (1.75.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (4.9.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core>=0.3.75->langchain_google_genai) (0.4.20)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core>=0.3.75->langchain_google_genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core>=0.3.75->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core>=0.3.75->langchain_google_genai) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.3.75->langchain_google_genai) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.75->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.24.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kush\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain_google_genai) (1.3.1)\n"
     ]
    },
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:285\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.client.batch_embed_contents(\n\u001b[32m    286\u001b[39m         BatchEmbedContentsRequest(requests=requests, model=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1438\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1437\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m response = rpc(\n\u001b[32m   1439\u001b[39m     request,\n\u001b[32m   1440\u001b[39m     retry=retry,\n\u001b[32m   1441\u001b[39m     timeout=timeout,\n\u001b[32m   1442\u001b[39m     metadata=metadata,\n\u001b[32m   1443\u001b[39m )\n\u001b[32m   1445\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    295\u001b[39m     target,\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    297\u001b[39m     sleep_generator,\n\u001b[32m    298\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    299\u001b[39m     on_error=on_error,\n\u001b[32m    300\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m         exc,\n\u001b[32m    158\u001b[39m         deadline,\n\u001b[32m    159\u001b[39m         sleep_iter,\n\u001b[32m    160\u001b[39m         error_list,\n\u001b[32m    161\u001b[39m         predicate,\n\u001b[32m    162\u001b[39m         on_error,\n\u001b[32m    163\u001b[39m         exception_factory,\n\u001b[32m    164\u001b[39m         timeout,\n\u001b[32m    165\u001b[39m     )\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = target()\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m vector_store = InMemoryVectorStore(embeddings)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Adding documents to vector store\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m document_ids = vector_store.add_documents(documents=all_splits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\vectorstores\\in_memory.py:200\u001b[39m, in \u001b[36mInMemoryVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, ids, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_documents\u001b[39m(\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m     **kwargs: Any,\n\u001b[32m    198\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    199\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     vectors = \u001b[38;5;28mself\u001b[39m.embedding.embed_documents(texts)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) != \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[32m    203\u001b[39m         msg = (\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mids must be the same length as texts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:290\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    289\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    291\u001b[39m     embeddings.extend([\u001b[38;5;28mlist\u001b[39m(e.values) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings])\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create a vector store\n",
    "# (CLASSROOM DISCUSSION: What are vector stores? What do we make them?)\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Adding documents to vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e746c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6363c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:324\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    318\u001b[39m     request: EmbedContentRequest = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    319\u001b[39m         text=text,\n\u001b[32m    320\u001b[39m         task_type=task_type,\n\u001b[32m    321\u001b[39m         title=title,\n\u001b[32m    322\u001b[39m         output_dimensionality=output_dimensionality,\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result: EmbedContentResponse = \u001b[38;5;28mself\u001b[39m.client.embed_content(request)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1306\u001b[39m, in \u001b[36mGenerativeServiceClient.embed_content\u001b[39m\u001b[34m(self, request, model, content, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1305\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m response = rpc(\n\u001b[32m   1307\u001b[39m     request,\n\u001b[32m   1308\u001b[39m     retry=retry,\n\u001b[32m   1309\u001b[39m     timeout=timeout,\n\u001b[32m   1310\u001b[39m     metadata=metadata,\n\u001b[32m   1311\u001b[39m )\n\u001b[32m   1313\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    295\u001b[39m     target,\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    297\u001b[39m     sleep_generator,\n\u001b[32m    298\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    299\u001b[39m     on_error=on_error,\n\u001b[32m    300\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m         exc,\n\u001b[32m    158\u001b[39m         deadline,\n\u001b[32m    159\u001b[39m         sleep_iter,\n\u001b[32m    160\u001b[39m         error_list,\n\u001b[32m    161\u001b[39m         predicate,\n\u001b[32m    162\u001b[39m         on_error,\n\u001b[32m    163\u001b[39m         exception_factory,\n\u001b[32m    164\u001b[39m         timeout,\n\u001b[32m    165\u001b[39m     )\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = target()\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# extract chunks which matches with your query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m search_results = vector_store.similarity_search_with_score(\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat is the role of Elon Musk right now?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     k = \u001b[32m10\u001b[39m\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\vectorstores\\in_memory.py:440\u001b[39m, in \u001b[36mInMemoryVectorStore.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, **kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    435\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     **kwargs: Any,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     embedding = \u001b[38;5;28mself\u001b[39m.embedding.embed_query(query)\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    442\u001b[39m         embedding,\n\u001b[32m    443\u001b[39m         k,\n\u001b[32m    444\u001b[39m         **kwargs,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:327\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    326\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result.embedding.values)\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "# extract chunks which matches with your query\n",
    "\n",
    "search_results = vector_store.similarity_search_with_score(\n",
    "    \"What is the role of Elon Musk right now?\",\n",
    "    k = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ba5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0ce6d",
   "metadata": {},
   "source": [
    "### What is RAG?\n",
    "Retrieve using semantic search and dump the similar chunks in the context of the prompt.\n",
    "LLM sees the question and retrieved docs in its prompt and generates tokens accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92552a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4152d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc_content = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(doc.page_content+\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m+\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m+\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (doc,score) \u001b[38;5;129;01min\u001b[39;00m search_results)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(doc_content)\n",
      "\u001b[31mNameError\u001b[39m: name 'search_results' is not defined"
     ]
    }
   ],
   "source": [
    "doc_content = \"\\n\\n\".join(doc.page_content+\"\\n\"+\"=\"*50+\"\\n\" for (doc,score) in search_results)\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b1451",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m model = init_chat_model(\u001b[33m\"\u001b[39m\u001b[33mllama-3.3-70b-versatile\u001b[39m\u001b[33m\"\u001b[39m, model_provider=\u001b[33m\"\u001b[39m\u001b[33mgroq\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m prompt_template = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know the answer, just say that you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know. Use three sentences maximum and keep the answer concise.\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33mAnswer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     11\u001b[39m response = model.invoke(prompt_template.format(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     context=doc_content,\n\u001b[32m     13\u001b[39m     question=\u001b[33m\"\u001b[39m\u001b[33mWhat is the role of Lion in the story?\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'doc_content' is not defined"
     ]
    }
   ],
   "source": [
    "# make the LLM read see the prompt, and analyse the retrieved document, and generate response\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(\n",
    "    context=doc_content,\n",
    "    question=\"What is the role of Elon Musk in the story?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better printing\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ceb2c",
   "metadata": {},
   "source": [
    "# RAG Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93cd5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split given book into 14 sub-documents.\n"
     ]
    },
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:285\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.client.batch_embed_contents(\n\u001b[32m    286\u001b[39m         BatchEmbedContentsRequest(requests=requests, model=\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1438\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1437\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m response = rpc(\n\u001b[32m   1439\u001b[39m     request,\n\u001b[32m   1440\u001b[39m     retry=retry,\n\u001b[32m   1441\u001b[39m     timeout=timeout,\n\u001b[32m   1442\u001b[39m     metadata=metadata,\n\u001b[32m   1443\u001b[39m )\n\u001b[32m   1445\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    295\u001b[39m     target,\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    297\u001b[39m     sleep_generator,\n\u001b[32m    298\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    299\u001b[39m     on_error=on_error,\n\u001b[32m    300\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = _retry_error_helper(\n\u001b[32m    157\u001b[39m         exc,\n\u001b[32m    158\u001b[39m         deadline,\n\u001b[32m    159\u001b[39m         sleep_iter,\n\u001b[32m    160\u001b[39m         error_list,\n\u001b[32m    161\u001b[39m         predicate,\n\u001b[32m    162\u001b[39m         on_error,\n\u001b[32m    163\u001b[39m         exception_factory,\n\u001b[32m    164\u001b[39m         timeout,\n\u001b[32m    165\u001b[39m     )\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = target()\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m vector_store = InMemoryVectorStore(embeddings)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Adding documents to vector store\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m document_ids = vector_store.add_documents(documents=all_splits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_core\\vectorstores\\in_memory.py:200\u001b[39m, in \u001b[36mInMemoryVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, ids, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_documents\u001b[39m(\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m     **kwargs: Any,\n\u001b[32m    198\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    199\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     vectors = \u001b[38;5;28mself\u001b[39m.embedding.embed_documents(texts)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) != \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[32m    203\u001b[39m         msg = (\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mids must be the same length as texts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_google_genai\\embeddings.py:290\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    289\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    291\u001b[39m     embeddings.extend([\u001b[38;5;28mlist\u001b[39m(e.values) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings])\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "# RAG summary\n",
    "\n",
    "# Read a doc\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"elonmusk.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split document into small chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split given book into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# embedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create a vector store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Adding documents to vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55042e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract chunks which matches with your query\n",
    "\n",
    "search_results = vector_store.similarity_search_with_score(\n",
    "    \"What is the role of Elon in the story?\",\n",
    "    k = 10\n",
    ")\n",
    "\n",
    "doc_content = \"\\n\\n\".join(doc.page_content+\"\\n\"+\"=\"*50+\"\\n\" for (doc,score) in search_results)\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94122ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m prompt_template = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know the answer, just say that you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know. Use three sentences maximum and keep the answer concise.\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mAnswer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m response = model.invoke(prompt_template.format(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     context=doc_content,\n\u001b[32m      8\u001b[39m     question=\u001b[33m\"\u001b[39m\u001b[33mWhat is the role of Elon in the story?\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown\n\u001b[32m     12\u001b[39m Markdown(response.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'doc_content' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(\n",
    "    context=doc_content,\n",
    "    question=\"What is the role of Elon in the story?\"))\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
